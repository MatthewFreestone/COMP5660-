{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1aad1b",
   "metadata": {},
   "source": [
    "# Assignment 2b Notebook: Automated Design of AI Agents with Genetic Programming\n",
    "This notebook will guide you through the process of implementing genetic programming (GP). This assignment will utilize components you implemented previously during Assignment Series 1, as well as the parse trees you implemented during Assignment 2a. From your previous assignments, copy over the most recent versions of the following files:\n",
    "* `base_evolution.py`\n",
    "* `selection.py`\n",
    "* `tree_genotype.py`\n",
    "\n",
    "**If you implemented your parse trees in a separate file, copy that file over as well.** In addition, you will need to import this file in the following code cell.\n",
    "\n",
    "As usual, be careful not to overwrite any of the other provided files, as we may have modified them between assignments. Make sure you are in the `EC-env` environment. If you are not, terminate the notebook server, run `conda activate EC-env`, and restart the notebook server.\n",
    "\n",
    "To begin the assignment, execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab84990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first cell has been executed!\n"
     ]
    }
   ],
   "source": [
    "# Configure this notebook to automatically reload modules as they're modified\n",
    "# https://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# %pip install snake-eyes-parser\n",
    "# %pip install pytest\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # hopefully stop any pedantic warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "import os\n",
    "import statistics\n",
    "import json\n",
    "from scipy import stats\n",
    "from snake_eyes import read_config\n",
    "from fitness import play_GPac\n",
    "from tree_genotype import *\n",
    "from selection import *\n",
    "from gpac_population_evaluation import *\n",
    "from genetic_programming import *\n",
    "from base_evolution import *\n",
    "from copy import deepcopy\n",
    "from search_runner import genetic_programming_search, run_and_log\n",
    "from yellow_search_runner import yellow_genetic_programming_search, yellow_run_and_log\n",
    "from red2_search_runner import red2_genetic_programming_search, red2_run_and_log\n",
    "from red3_search_runner import red3_genetic_programming_search, red3_run_and_log\n",
    "from red4_search_runner import red4_genetic_programming_search, red4_run_and_log\n",
    "from red5_search_runner import red5_genetic_programming_search, red5_run_and_log\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "print('The first cell has been executed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(41)\n",
    "a = ParseTree(terminals=['t1', 't2', 't3', 't4'], nonterminals=['*', '+', '-', '/', 'RAND'])\n",
    "a.initialize(method='grow', depth_limit=5)\n",
    "print(a)\n",
    "n, d = a.getNthNode(5)\n",
    "print(n,d)\n",
    "# a.pruneToDepth(3)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6045fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1_0000):\n",
    "# random.seed(i)\n",
    "config = read_config('configs/2b_green_config.txt', globals(), locals())\n",
    "a = TreeGenotype(**config['problem'])\n",
    "a.genes = ParseTree(**config['problem'])\n",
    "a.genes.initialize(2, 'grow', (-2,2))\n",
    "b = TreeGenotype(**config['problem'])\n",
    "b.genes = ParseTree(**config['problem'])\n",
    "b.genes.initialize(1, 'grow', (-2,2))\n",
    "# print(a.genes)\n",
    "# print(b.genes)\n",
    "res = a.recombine(b, 10)\n",
    "print(res.genes, res.genes.maxDepth())\n",
    "\n",
    "res = res.mutate(10)\n",
    "# print(res.genes)\n",
    "\n",
    "res = res.recombine(a,10)\n",
    "# checker(res.genes.root.left)\n",
    "# checker(res.genes.root.right)\n",
    "# print(res.genes)\n",
    "\n",
    "def checker(curr):\n",
    "    if curr:\n",
    "        assert curr.parent is not None\n",
    "        checker(curr.left)\n",
    "        checker(curr.right)\n",
    "\n",
    "checker(res.genes.root.left)\n",
    "checker(res.genes.root.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc276e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config('configs/2b_green_config.txt', globals(), locals())\n",
    "\n",
    "# Initialize a population of 100 individuals.\n",
    "population = TreeGenotype.initialization(4, **config['problem'])\n",
    "\n",
    "# Perform recombination to make 50 children.\n",
    "children = list()\n",
    "for idx in range(0, len(population), 2):\n",
    "    child = population[idx].recombine(population[idx+1], **config['problem'])\n",
    "    children.append(child)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab68443",
   "metadata": {},
   "source": [
    "## Recombination\n",
    "Assuming you've correctly implemented population initialization in Assignment 2a, we'll begin this assignment by implementing recombination with subtree crossover. Like Assignment Series 1, your recombination method should take a mate as input and recombine the `genes` members of `self` and `mate`, assign the recombined tree to the `genes` member variable of `child`, and then return `child`.\n",
    "\n",
    "Recall that this assignment series involves a configurable maximum depth, specified in the config files, and this applies to children produced with recombination and mutation. How you do this is up to you, but you must guarantee that the children your algorithms produce do not go beyond the configured max depth.\n",
    "\n",
    "How you implement subtree crossover in practice depends on your implementation of the parse tree genotype and is thus open-ended. Implement the `TreeGenotype.recombine` function in `tree_genotype.py`, then test your implementation by executing the following cell. This cell will use the same tree checker as Assignment 2a to verify that your trees are structurally correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aebb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config('configs/2b_green_config.txt', globals(), locals())\n",
    "\n",
    "# Initialize a population of 100 individuals.\n",
    "population = TreeGenotype.initialization(100, **config['problem'])\n",
    "\n",
    "# Perform recombination to make 50 children.\n",
    "children = list()\n",
    "for idx in range(0, len(population), 2):\n",
    "    child = population[idx].recombine(population[idx+1], **config['problem'])\n",
    "    children.append(child)\n",
    "\n",
    "# Save the child trees to files.\n",
    "os.makedirs('tree_tests/', exist_ok=True)\n",
    "for idx, individual in enumerate(children):\n",
    "    with open(f'tree_tests/tree{idx}r.txt', 'w') as f:\n",
    "        f.write(individual.to_string())\n",
    "\n",
    "!python tree_check.py tree_tests/tree*r.txt\n",
    "\n",
    "del config, population, children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b802db",
   "metadata": {},
   "source": [
    "## Mutation\n",
    "Using your parse tree genotype, implement mutation in `TreeGenotype.mutate` and test your implementation by executing the next cell. You can use the grow method that you implemented in the previous assignment to grow a new subtree, replacing an existing randomly-selected subtree. This is called *subtree mutation* in GP literature. Note that you should make sure you don't select the root, which would cause you to grow an entirely new tree. And don't forget to enforce max tree depth like in recombination!\n",
    "\n",
    "You can also implement mutation in a number of other ways. Particularly, *point mutation* is also common in GP literature and is typically regarded as a performant GP mutation method. In context of the trees we're using here, this would be comparable to a random reset for a small number of the primitives in the tree without modifying the tree structure. That is, you could pick some (preferably small) subset of the nodes in a tree, then randomly select a new primitive for each of the selected nodes (but make sure that terminal nodes stay terminal nodes, and nonterminal nodes stay nonterminal nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67dc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config('configs/2b_green_config.txt', globals(), locals())\n",
    "random.seed(43)\n",
    "# Initialize a population of 100 individuals.\n",
    "population = TreeGenotype.initialization(1, **config['problem'])\n",
    "\n",
    "# Perform recombination to make 50 children.\n",
    "print(population[0].genes)\n",
    "# print(population[0].genes.getNthNode(n)[0])\n",
    "# population[0].genes.setNthNode(n, TreeNode('FOO'))\n",
    "\n",
    "# print(population[0].genes)\n",
    "# print(population[0].genes.getNthNode(n)[0])\n",
    "mutant = population[0].mutate(depth_limit=4)\n",
    "print(mutant.genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352638f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config('configs/2b_green_config.txt', globals(), locals())\n",
    "\n",
    "# Initialize a population of 50 individuals.\n",
    "population = TreeGenotype.initialization(50, **config['problem'])\n",
    "\n",
    "# Perform mutation to make 50 children.\n",
    "children = list()\n",
    "for idx in range(len(population)):\n",
    "    child = population[idx].mutate(**config['problem'])\n",
    "    children.append(child)\n",
    "\n",
    "# Save the child trees to files.\n",
    "os.makedirs('tree_tests/', exist_ok=True)\n",
    "for idx, individual in enumerate(children):\n",
    "    with open(f'tree_tests/tree{idx}m.txt', 'w') as f:\n",
    "        f.write(individual.to_string())\n",
    "\n",
    "!python tree_check.py tree_tests/tree*m.txt\n",
    "\n",
    "del config, population, children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b505608",
   "metadata": {},
   "source": [
    "## Implementing Genetic Programming\n",
    "By this point, you should have a complete implementation of your genotype and we can turn our attention to implementing the complete GP algorithm.\n",
    "\n",
    "### Evaluation and Parsimony Pressure\n",
    "Recall from the lecture and videos by Dr. Koza that tree GP, using the recombination and mutation methods we have implemented here, tends to produce larger and larger trees. Therefore, most GP implementations provide some mechanism to discourage this \"bloat\".\n",
    "\n",
    "You will implement a parsimony penalty to discourage your GP from producing unnecessarily large trees (in addition to the depth limit you already have). This parsimony pressure mechanism penalizes the fitness of a solution based on its size. We will apply this using a very similar calculation to the penalty function in Assignment 1c, using the size of the tree rather than a number of constraint violations:\n",
    "\n",
    "`fitness = base_fitness - size * parsimony_coefficient`\n",
    "\n",
    "Where `base_fitness` is the solution's fitness according to the fitness function, `size` is some measure of the tree's size, `parsimony_coefficient` is from your config file, and `fitness` is the penalized fitness. The two most obvious metrics of tree size are max depth and node count, but you are encouraged to experiment with different metrics.\n",
    "\n",
    "Just like Assignment 1c, **it is not meaningful to compare penalized fitness with unpenalized fitness**. We require that you use base fitness for your analysis, and penalized fitness for evolution (which will happen automatically if you store this in the `fitness` member variable, just like in Assignment 1c).\n",
    "\n",
    "In the file `gpac_population_evaluation.py`, implement the `base_population_evaluation` function that performs fitness evaluations on an input population and assigns the `base_fitness`, parsimony-penalized `fitness`, and `log` members to each individual. Then, test your implementation by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d158b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config('configs/2b_green_config.txt', globals(), locals())\n",
    "\n",
    "# Initialize a population.\n",
    "population = TreeGenotype.initialization(100, **config['problem'])\n",
    "\n",
    "# Note explicitly initializing to None is unnecessary in general.\n",
    "# We only do it here to test that you've actually set these values.\n",
    "for individual in population:\n",
    "    individual.fitness = None\n",
    "    individual.base_fitness = None\n",
    "    individual.log = None\n",
    "\n",
    "# Calling your function to test things out.\n",
    "base_population_evaluation(population, **config['fitness_kwargs'], **config['game'])\n",
    "\n",
    "missing = len([True for individual in population if individual.fitness is None])\n",
    "base_missing = len([True for individual in population if individual.base_fitness is None])\n",
    "log_missing = len([True for individual in population if individual.log is None])\n",
    "\n",
    "if missing or base_missing or log_missing:\n",
    "    print('Some individuals had some members left unassigned.')\n",
    "    print('Make sure you assign all required members to all individuals, then run this cell again.')\n",
    "    print('Individuals with unassigned (penalized) fitness:', missing)\n",
    "    print('Individuals with unassigned base fitness:', base_missing)\n",
    "    print('Individuals with unassigned log:', log_missing)\n",
    "\n",
    "else:\n",
    "    fitnesses = [individual.fitness for individual in population]\n",
    "    print('Average fitness of population:', statistics.mean(fitnesses))\n",
    "    print('Best fitness in population:', max(fitnesses))\n",
    "    base_fitnesses = [individual.base_fitness for individual in population]\n",
    "    print('Average unpenalized (base) fitness of population:', statistics.mean(base_fitnesses))\n",
    "    print('Best unpenalized (base) fitness in population:', max(base_fitnesses))\n",
    "\n",
    "    best_log = max(population, key=lambda ind:ind.base_fitness).log\n",
    "    game_log_path = 'example_game.txt'\n",
    "    with open(game_log_path, 'w') as f:\n",
    "        [f.write(f'{line}\\n') for line in best_log]\n",
    "    print('The log of the most fit individual was written to', game_log_path)\n",
    "    \n",
    "    del fitnesses, base_fitnesses, best_log\n",
    "\n",
    "del config, population, missing, base_missing, log_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe93122",
   "metadata": {},
   "source": [
    "### Child Generation\n",
    "With fitness evaluation implemented, you can now implement child generation within the `GeneticProgrammingPopulation` class. This class inherits from the `BaseEvolutionPopulation` class you used in Assignment Series 1, and will use the inherited initialization and survival selection methods without modification. The only method that you need to implement, then, is `generate_children`. This will override the same method in the base class.\n",
    "\n",
    "Recall that, typically, mutation in GP is mutually exclusive with recombination. That is, mutation in GP is usually used to directly produce children by mutating a copy of the parent. Therefore, you need to implement a GP-specific version of `generate_children` in the `GeneticProgrammingPopulation` class, which will override the version you previously implemented. The implementation should be broadly similar, except that you need to decide whether or not an individual should be mutated (rather than recombined) before recombination occurs, and either do one or the other (but not both).\n",
    "\n",
    "Once complete, test your implementation using the following cell, which will initialize your EA, as well as making one generation of children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11352eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config('configs/2b_green_config.txt', globals(), locals())\n",
    "\n",
    "# Full initialization of your GP population.\n",
    "example_ea = GeneticProgrammingPopulation(**config['ea'], **config)\n",
    "\n",
    "# Evaluate the initial population.\n",
    "base_population_evaluation(example_ea.population, **config['fitness_kwargs'], **config['game'])\n",
    "example_ea.evaluations = len(example_ea.population)\n",
    "\n",
    "# Print statements for tutorialization.\n",
    "print('Number of fitness evaluations:', example_ea.evaluations)\n",
    "fitnesses = [individual.fitness for individual in example_ea.population]\n",
    "print('Average fitness of population:', statistics.mean(fitnesses))\n",
    "print('Best fitness in population:', max(fitnesses))\n",
    "base_fitnesses = [individual.base_fitness for individual in example_ea.population]\n",
    "print('Average unpenalized (base) fitness of population:', statistics.mean(base_fitnesses))\n",
    "print('Best unpenalized (base) fitness in population:', max(base_fitnesses))\n",
    "\n",
    "# Make a generation of children.\n",
    "children = example_ea.generate_children()\n",
    "\n",
    "# Evaluate the children.\n",
    "base_population_evaluation(children, **config['fitness_kwargs'], **config['game'])\n",
    "example_ea.evaluations += len(children)\n",
    "\n",
    "# Print statements for tutorialization.\n",
    "print('Number of fitness evaluations:', example_ea.evaluations)\n",
    "child_fitnesses = [individual.fitness for individual in children]\n",
    "print('Average fitness of children:', statistics.mean(child_fitnesses))\n",
    "print('Best fitness in children:', max(child_fitnesses))\n",
    "base_child_fitnesses = [individual.base_fitness for individual in children]\n",
    "print('Average unpenalized (base) fitness of children:', statistics.mean(base_child_fitnesses))\n",
    "print('Best unpenalized (base) fitness in children:', max(base_child_fitnesses))\n",
    "\n",
    "# Print children trees to files.\n",
    "os.makedirs('tree_tests/', exist_ok=True)\n",
    "for idx, individual in enumerate(children):\n",
    "    with open(f'tree_tests/tree{idx}c.txt', 'w') as f:\n",
    "        f.write(individual.to_string())\n",
    "\n",
    "!python tree_check.py tree_tests/tree*c.txt\n",
    "\n",
    "del config, example_ea, children, fitnesses, base_fitnesses, child_fitnesses, base_child_fitnesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f073c8",
   "metadata": {},
   "source": [
    "## Single Run Experiment\n",
    "At this point, you should have everything you need to implement a full GP search to evolve GPac controllers! Now put all the components together and implement a function to conduct a single full run in the next cell.\n",
    "\n",
    "**Note**: Just like in Assignment 2a, you should consider that the game logs (now stored in the `TreeGenotype.log` member variable) can consume a large amount of memory. As a result, it may be a good idea to manually remove logs from individuals that you know will not be necessary for analysis (i.e., anyone except the global-best individual). The following code should suffice for the GREEN and YELLOW deliverables, but may not be applicable for some of the RED deliverables:\n",
    "\n",
    "```py\n",
    "best_individual = None\n",
    "for individual in population:\n",
    "    if best_individual is None:\n",
    "        best_individual = individual\n",
    "    elif individual.base_fitness > best_individual.base_fitness:\n",
    "        del best_individual.log\n",
    "        best_individual = individual\n",
    "    elif individual.base_fitness < best_individual.base_fitness:\n",
    "        del individual.log\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling your function to test things out.\n",
    "# import cProfile\n",
    "# with cProfile.Profile() as pr:\n",
    "# res = genetic_programming_search(100, 'configs/2b_green_config.txt')\n",
    "    # pr.print_stats(sort='cumtime')\n",
    "run_and_log(genetic_programming_search, 100, 'configs/2b_green_config.txt', './testout.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820444e",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "As is the case with all EAs, tuning your GP algorithm is very important for good performance! In this case, it's also a graded component of the Algorithmic section of this assignment. Like Assignment 1b, you are expected to tune your algorithm parameters such that your full GP search outperforms the random search used in Assignment 2a (note that the default parameters are intentionally bad). As was the case with Assignment 1b, hand-tuning your parameters using a couple runs at a time is good enough for this class. While tuning, it can be helpful to ask yourself the following:\n",
    " * How many generations of evolution will occur based on $\\mu$ and $\\lambda$? Is this enough generations for evolution to find good solutions?\n",
    " * Is premature convergence occuring (e.g., max fitness stops increasing at a mediocre value)?\n",
    " * Is a loss of diversity preventing improvement (e.g., mean and best fitness converge to very similar values)?\n",
    " * Have the problem instance or search space parameters been manipulated in a way that invalidates a comparison with ramped half-and-half (i.e., avoid manipulating the game parameters and tree depth limits)?\n",
    " * Is the parsimony coefficient tuned well? (Note: you'll need to consider this explicitly if attempting the YELLOW deliverable)\n",
    "\n",
    "Use the following cell to tune your parameters. Make sure to include the changes to your config in your final submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d57bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 10\n",
    "number_evaluations = 5000\n",
    "config_filename = 'configs/2b_green_config.txt'\n",
    "log_directory = 'data/2b/green/logs/tuning/'\n",
    "\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Tuning runs can be called here\n",
    "def one_run(i):\n",
    "    run_and_log(genetic_programming_search, number_evaluations, config_filename, log_directory+str(i)+\".json\")\n",
    "with Pool() as pool:\n",
    "    pool.map(one_run, range(number_runs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b03b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_directory = 'data/2b/green/logs/tuning/'\n",
    "evals = None\n",
    "total_penalized_means = None\n",
    "total_penalized_maxes = None\n",
    "total_base_means = None\n",
    "total_base_maxes = None\n",
    "n = 0\n",
    "for file in os.listdir(log_directory):\n",
    "    data = None\n",
    "    with open(log_directory + file, 'r') as f:\n",
    "        print(log_directory + file, end=\" \")\n",
    "        data = json.load(f)\n",
    "        print(data['best_fitness'])\n",
    "        if not evals:\n",
    "            evals = data['evaluations']\n",
    "            total_penalized_means = data['mean_penalized_fitness']\n",
    "            total_penalized_maxes = data['best_penalized_fitness']\n",
    "            total_base_means = data['mean_base_fitness']\n",
    "            total_base_maxes = data['best_base_fitness']\n",
    "        else:\n",
    "            for i in range(len(evals)):\n",
    "                total_penalized_means[i] += data['mean_penalized_fitness'][i]\n",
    "                total_penalized_maxes[i] += data['best_penalized_fitness'][i]\n",
    "                total_base_means[i] += data['mean_base_fitness'][i]\n",
    "                total_base_maxes[i] += data['best_base_fitness'][i]\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_penalized_means = [t/n for t in total_penalized_means]\n",
    "average_penalized_maxes = [t/n for t in total_penalized_maxes]\n",
    "average_base_means = [t/n for t in total_base_means]\n",
    "average_base_maxes = [t/n for t in total_base_maxes]\n",
    "evals = evals\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evals, average_penalized_maxes, 'g-')\n",
    "ax.plot(evals, average_penalized_means, 'g--')\n",
    "ax.plot(evals, average_base_maxes, 'b-')\n",
    "ax.plot(evals, average_base_means, 'b--')\n",
    "ax.set(xlabel = 'Evaluations', ylabel = 'Fitness',\\\n",
    "       title = f'2b Green Tuning: Evaluations vs Population Fitness Averaged Across {n} Runs')\n",
    "ax.legend(['Max Penalized Fitness', 'Mean Penalized Fitness', 'Max Base Fitness', 'Mean Base Fitness'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3fbce",
   "metadata": {},
   "source": [
    "## Multi-Run Experiments\n",
    "Implement a full 10-run experiment with 5,000 fitness evaluations per run. For each generation, including the initial population, log the average and best fitness and base fitness of the current population (so, 4 values in total) along with the evaluation count. For each run, log the global best base fitness encountered for statistical analysis. For the individual with the global best base fitness from each run, log the parse tree and game log for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_runs = 10\n",
    "number_evaluations = 5000\n",
    "config_filename = 'configs/2b_green_config.txt'\n",
    "log_directory = 'data/2b/green/logs/'\n",
    "\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Tuning runs can be called here\n",
    "def one_run(i):\n",
    "    run_and_log(genetic_programming_search, number_evaluations, config_filename, log_directory+str(i)+\".json\")\n",
    "with Pool() as pool:\n",
    "    pool.map(one_run, range(number_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'data/2b/green/logs/'\n",
    "\n",
    "# for plotting\n",
    "evals = None\n",
    "total_penalized_means = None\n",
    "total_penalized_maxes = None\n",
    "total_base_means = None\n",
    "total_base_maxes = None\n",
    "\n",
    "# required and starts\n",
    "run_logs = []\n",
    "best_scores = []\n",
    "best_genes = []\n",
    "game_logs = []\n",
    "\n",
    "n = 0\n",
    "for file in os.listdir(log_directory):\n",
    "    if not file.endswith('.json'):\n",
    "        continue\n",
    "    data = None\n",
    "    with open(log_directory + file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        run_logs.append(data['gp_log'])\n",
    "        best_scores.append(data['best_fitness'])\n",
    "        best_genes.append(data['best_genes'])\n",
    "        game_logs.append(data['best_game_log'])\n",
    "        if not evals:\n",
    "            evals = data['evaluations']\n",
    "            total_penalized_means = data['mean_penalized_fitness']\n",
    "            total_penalized_maxes = data['best_penalized_fitness']\n",
    "            total_base_means = data['mean_base_fitness']\n",
    "            total_base_maxes = data['best_base_fitness']\n",
    "        else:\n",
    "            for i in range(len(evals)):\n",
    "                total_penalized_means[i] += data['mean_penalized_fitness'][i]\n",
    "                total_penalized_maxes[i] += data['best_penalized_fitness'][i]\n",
    "                total_base_means[i] += data['mean_base_fitness'][i]\n",
    "                total_base_maxes[i] += data['best_base_fitness'][i]\n",
    "    n += 1\n",
    "\n",
    "best_run = best_scores.index(max(best_scores))\n",
    "print(\"Best run was\", best_run, \"with genes \\n\", best_genes[best_run])\n",
    "with open('data/2b/green/best_game.txt', 'w') as f:\n",
    "    f.write('\\n'.join(game_logs[best_run]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30911156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign your EA's log member variables here\n",
    "# It should be a list containing the log from the EA in each run\n",
    "run_logs = run_logs\n",
    "\n",
    "# Writing the logs to files\n",
    "os.makedirs('data/2b/green/logs/', exist_ok=True)\n",
    "for i in range(len(run_logs)):\n",
    "    with open('data/2b/green/logs/' + str(i+1) + '.txt', 'w') as f:\n",
    "        f.write(''.join([entry + '\\n' for entry in run_logs[i]]))\n",
    "\n",
    "# Assign your data for statistical analysis to this variable\n",
    "# It should be a list of the highest fitness values seen per run\n",
    "stats_data = best_scores\n",
    "\n",
    "# Writing your statistical data to a file\n",
    "with open('data/2b/green/statistics.txt', 'w') as f:\n",
    "    for result in stats_data:\n",
    "        f.write(str(result) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b015188",
   "metadata": {},
   "source": [
    "## Report\n",
    "Now you should have the data required to complete the report described in the assignment document. There are components of the report you need to complete independently, but this notebook will walk you through plotting and statistical analysis.\n",
    "\n",
    "### Statistical Analysis\n",
    "Statistical analysis should consist of a comparison between global best per-run **base** fitness values of your GP searches with the data you generated during your experiment in Assignment 2a. That data should have been saved in your Assignment 2a repository under `data/2a/green/statistics.txt`. If you have not already done so, copy that file to this assignment's `data` subdirectory. Then you can run the below cell and interpret the results. Recall that we specify $\\alpha = 0.05$ for these tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f2e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/2b/green/statistics.txt', 'r') as f:\n",
    "    gp_data = [float(line) for line in f.readlines()]\n",
    "\n",
    "print('2b data mean:', statistics.mean(gp_data))\n",
    "print('2b data stdv:', statistics.stdev(gp_data))\n",
    "\n",
    "# Reading the data from your 2a experiment\n",
    "with open('data/2a/green/statistics.txt', 'r') as f:\n",
    "    random_data = [float(line) for line in f.readlines()]\n",
    "\n",
    "print('2a data mean:', statistics.mean(random_data))\n",
    "print('2a data stdv:', statistics.stdev(random_data))\n",
    "\n",
    "test_result = stats.ttest_ind(random_data, gp_data, equal_var=False)\n",
    "print('p-value:', test_result.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa406e42",
   "metadata": {},
   "source": [
    "### Evals-vs-fitness Plot\n",
    "Using the data you've collected from your 10 run experiment, analyze your fitness values to find the mean and maximum fitness and base fitness at each generation averaged over your 10 runs, as you should have done several times now.\n",
    "\n",
    "Using this data, plot the number of evaluations (not generations) versus the mean and maximum fitness and base fitness averaged over 10 runs. The following cell has code to generate the plot, but you need to calculate and set the x and y values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125d96b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Store the y-values for mean fitness in average_penalized_means,\n",
    "and the y-values for best fitness in average_penalized_maxes.\n",
    "Do similar for average_base_means and average_base_maxes.\n",
    "x-values should represent the number of **evaluations**, NOT **generations**.\n",
    "Store the evaluation counts in the evals variable.\n",
    "'''\n",
    "average_penalized_means = [t/n for t in total_penalized_means]\n",
    "average_penalized_maxes = [t/n for t in total_penalized_maxes]\n",
    "average_base_means = [t/n for t in total_base_means]\n",
    "average_base_maxes = [t/n for t in total_base_maxes]\n",
    "evals = evals\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evals, average_penalized_maxes, 'g-')\n",
    "ax.plot(evals, average_penalized_means, 'g--')\n",
    "ax.plot(evals, average_base_maxes, 'b-')\n",
    "ax.plot(evals, average_base_means, 'b--')\n",
    "ax.set(xlabel = 'Evaluations', ylabel = 'Fitness',\\\n",
    "       title = '2b Green Experiment: Evaluations vs Population Fitness Averaged Across 10 Runs')\n",
    "ax.legend(['Max Penalized Fitness', 'Mean Penalized Fitness', 'Max Base Fitness', 'Mean Base Fitness'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba0ba4a",
   "metadata": {},
   "source": [
    "# YELLOW Deliverable\n",
    "Now that you've completed the GREEN deliverable and conducted an experiment with one choice of parsimony metric (most likely max depth or node count), experiment with a different metric and re-tune the parsimony coefficient. You are welcome to reuse the other parameters you used in your GREEN configuration, but some attempt at tuning the parsimony coefficient to account for the new parsimony metric is required. Implement your new parsimony calculation in the `base_population_evaluation` function under the `yellow` section.\n",
    "\n",
    "Use the following 2 cells to tune and conduct a 10-run experiment, respectively. Add more cells below these for your analysis, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f36354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "from yellow_search_runner import yellow_genetic_programming_search, yellow_run_and_log\n",
    "number_runs = 3\n",
    "number_evaluations = 500\n",
    "config_filename = 'configs/2b_yellow_config.txt'\n",
    "log_directory = 'data/2b/yellow/logs/tuning/'\n",
    "\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Tuning runs can be called here\n",
    "def one_run(i):\n",
    "    yellow_run_and_log(yellow_genetic_programming_search, number_evaluations, config_filename, log_directory+str(i)+\".json\")\n",
    "with Pool() as pool:\n",
    "    pool.map(one_run, range(number_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'data/2b/yellow/logs/tuning/'\n",
    "evals = None\n",
    "total_penalized_means = None\n",
    "total_penalized_maxes = None\n",
    "total_base_means = None\n",
    "total_base_maxes = None\n",
    "\n",
    "total_min_depth = None\n",
    "total_avg_depth = None\n",
    "total_max_depth = None\n",
    "\n",
    "n = 0\n",
    "for file in os.listdir(log_directory):\n",
    "    data = None\n",
    "    with open(log_directory + file, 'r') as f:\n",
    "        print(log_directory + file, end=\" \")\n",
    "        data = json.load(f)\n",
    "        print(data['best_fitness'])\n",
    "        if not evals:\n",
    "            evals = data['evaluations']\n",
    "            total_penalized_means = data['mean_penalized_fitness']\n",
    "            total_penalized_maxes = data['best_penalized_fitness']\n",
    "            total_base_means = data['mean_base_fitness']\n",
    "            total_base_maxes = data['best_base_fitness']\n",
    "            total_min_depth = data['min_depth']\n",
    "            total_avg_depth = data['avg_depth']\n",
    "            total_max_depth = data['max_depth']\n",
    "        else:\n",
    "            for i in range(len(evals)):\n",
    "                total_penalized_means[i] += data['mean_penalized_fitness'][i]\n",
    "                total_penalized_maxes[i] += data['best_penalized_fitness'][i]\n",
    "                total_base_means[i] += data['mean_base_fitness'][i]\n",
    "                total_base_maxes[i] += data['best_base_fitness'][i]\n",
    "                total_min_depth[i] += data['min_depth'][i]\n",
    "                total_avg_depth[i] += data['avg_depth'][i]\n",
    "                total_max_depth[i] += data['max_depth'][i]\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ea4e6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_penalized_means = [t/n for t in total_penalized_means]\n",
    "average_penalized_maxes = [t/n for t in total_penalized_maxes]\n",
    "average_base_means = [t/n for t in total_base_means]\n",
    "average_base_maxes = [t/n for t in total_base_maxes]\n",
    "\n",
    "average_min_depth = [t/n for t in total_min_depth]\n",
    "average_avg_depth = [t/n for t in total_avg_depth]\n",
    "average_max_depth = [t/n for t in total_max_depth]\n",
    "evals = evals\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evals, average_penalized_maxes, 'g-')\n",
    "ax.plot(evals, average_penalized_means, 'g--')\n",
    "ax.plot(evals, average_base_maxes, 'b-')\n",
    "ax.plot(evals, average_base_means, 'b--')\n",
    "ax.set(xlabel = 'Evaluations', ylabel = 'Fitness',\\\n",
    "       title = f'2b Green Tuning: Evaluations vs Population Fitness Averaged Across {n} Runs')\n",
    "ax.legend(['Max Penalized Fitness', 'Mean Penalized Fitness', 'Max Base Fitness', 'Mean Base Fitness'])\n",
    "ax2 = ax.twinx()\n",
    "ax2.set(ylabel='Tree Depth')\n",
    "ax2.plot(evals, average_min_depth, 'r*')\n",
    "ax2.plot(evals, average_avg_depth, 'r--')\n",
    "ax2.plot(evals, average_max_depth, 'r--')\n",
    "ax2.legend(['Min Depth', 'Average Depth', 'Max Depth'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4311f6d6",
   "metadata": {},
   "source": [
    "### Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 10\n",
    "number_evaluations = 5000\n",
    "config_filename = 'configs/2b_yellow_config.txt'\n",
    "log_directory = 'data/2b/yellow/logs/'\n",
    "\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Tuning runs can be called here\n",
    "def one_run(i):\n",
    "    yellow_run_and_log(yellow_genetic_programming_search, number_evaluations, config_filename, log_directory+str(i)+\".json\")\n",
    "with Pool() as pool:\n",
    "    pool.map(one_run, range(number_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f42673",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'data/2b/yellow/logs/'\n",
    "\n",
    "# for plotting\n",
    "evals = None\n",
    "total_penalized_means = None\n",
    "total_penalized_maxes = None\n",
    "total_base_means = None\n",
    "total_base_maxes = None\n",
    "\n",
    "total_min_depth = None\n",
    "total_avg_depth = None\n",
    "total_max_depth = None\n",
    "\n",
    "# required and starts\n",
    "run_logs = []\n",
    "best_scores = []\n",
    "best_genes = []\n",
    "game_logs = []\n",
    "\n",
    "n = 0\n",
    "for file in os.listdir(log_directory):\n",
    "    if not file.endswith('.json'):\n",
    "        continue\n",
    "    data = None\n",
    "    with open(log_directory + file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        run_logs.append(data['gp_log'])\n",
    "        best_scores.append(data['best_fitness'])\n",
    "        best_genes.append(data['best_genes'])\n",
    "        game_logs.append(data['best_game_log'])\n",
    "        \n",
    "        if not evals:\n",
    "            evals = data['evaluations']\n",
    "            total_penalized_means = data['mean_penalized_fitness']\n",
    "            total_penalized_maxes = data['best_penalized_fitness']\n",
    "            total_base_means = data['mean_base_fitness']\n",
    "            total_base_maxes = data['best_base_fitness']\n",
    "            total_min_depth = data['min_depth']\n",
    "            total_avg_depth = data['avg_depth']\n",
    "            total_max_depth = data['max_depth']\n",
    "        else:\n",
    "            for i in range(len(evals)):\n",
    "                total_penalized_means[i] += data['mean_penalized_fitness'][i]\n",
    "                total_penalized_maxes[i] += data['best_penalized_fitness'][i]\n",
    "                total_base_means[i] += data['mean_base_fitness'][i]\n",
    "                total_base_maxes[i] += data['best_base_fitness'][i]\n",
    "                total_min_depth[i] += data['min_depth'][i]\n",
    "                total_avg_depth[i] += data['avg_depth'][i]\n",
    "                total_max_depth[i] += data['max_depth'][i]\n",
    "    n += 1\n",
    "\n",
    "best_run = best_scores.index(max(best_scores))\n",
    "print(\"Best run was\", best_run, \"with score\", max(best_scores), \"and genes \\n\", best_genes[best_run])\n",
    "with open('data/2b/yellow/best_game.txt', 'w') as f:\n",
    "    f.write('\\n'.join(game_logs[best_run]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign your EA's log member variables here\n",
    "# It should be a list containing the log from the EA in each run\n",
    "run_logs = run_logs\n",
    "\n",
    "# Writing the logs to files\n",
    "os.makedirs('data/2b/yellow/logs/', exist_ok=True)\n",
    "for i in range(len(run_logs)):\n",
    "    with open('data/2b/yellow/logs/' + str(i+1) + '.txt', 'w') as f:\n",
    "        f.write(''.join([entry + '\\n' for entry in run_logs[i]]))\n",
    "\n",
    "# Assign your data for statistical analysis to this variable\n",
    "# It should be a list of the highest fitness values seen per run\n",
    "stats_data = best_scores\n",
    "\n",
    "# Writing your statistical data to a file\n",
    "with open('data/2b/yellow/statistics.txt', 'w') as f:\n",
    "    for result in stats_data:\n",
    "        f.write(str(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/2b/yellow/statistics.txt', 'r') as f:\n",
    "    gp_data = [float(line) for line in f.readlines()]\n",
    "\n",
    "print('Yellow data mean:', statistics.mean(gp_data))\n",
    "print('Yellow data stdv:', statistics.stdev(gp_data))\n",
    "\n",
    "with open('data/2b/green/statistics.txt', 'r') as f:\n",
    "    random_data = [float(line) for line in f.readlines()]\n",
    "\n",
    "print('Green data mean:', statistics.mean(random_data))\n",
    "print('Green data stdv:', statistics.stdev(random_data))\n",
    "\n",
    "test_result = stats.ttest_ind(random_data, gp_data, equal_var=False)\n",
    "print('p-value:', test_result.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea54ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_penalized_means = [t/n for t in total_penalized_means]\n",
    "average_penalized_maxes = [t/n for t in total_penalized_maxes]\n",
    "average_base_means = [t/n for t in total_base_means]\n",
    "average_base_maxes = [t/n for t in total_base_maxes]\n",
    "evals = evals\n",
    "\n",
    "average_min_depth = [t/n for t in total_min_depth]\n",
    "average_avg_depth = [t/n for t in total_avg_depth]\n",
    "average_max_depth = [t/n for t in total_max_depth]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evals, average_penalized_maxes, 'g-')\n",
    "ax.plot(evals, average_penalized_means, 'g--')\n",
    "ax.plot(evals, average_base_maxes, 'b-')\n",
    "ax.plot(evals, average_base_means, 'b--')\n",
    "ax.set(xlabel = 'Evaluations', ylabel = 'Fitness',\\\n",
    "       title = f'2b Yellow: Evaluations vs Population Fitness Averaged Across {n} Runs')\n",
    "ax.legend(['Max Penalized Fitness', 'Mean Penalized Fitness', 'Max Base Fitness', 'Mean Base Fitness'])\n",
    "ax2 = ax.twinx()\n",
    "ax2.set(ylabel='Tree Depth')\n",
    "ax2.plot(evals, average_min_depth, 'r--')\n",
    "ax2.plot(evals, average_avg_depth, 'r-.')\n",
    "ax2.plot(evals, average_max_depth, 'r-')\n",
    "ax2.legend(['Min Depth', 'Average Depth', 'Max Depth'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ede87f",
   "metadata": {},
   "source": [
    "# RED Deliverables (Bonus)\n",
    "Bonus RED deliverables are typically somewhat open-ended, but this is particularly true in Assignment Series 2. As such, you may encounter unanticipated design decisions and obstacles that you should figure out how to overcome. TA assistance is available to answer questions, though you are expected to experiment and investigate solutions/answers prior to seeking this assistance. You are welcome to re-use tuned parameters between these experiments instead of conducting more rigorous tuning, though you may observe more interesting results with per-deliverable tuning.\n",
    "\n",
    "For each RED deliverable you attempt, **do not overwrite or break compatibility with your previous experimentation**: create a new notebook cell below, create a new config file, and implement the relevant section in the `base_population_evaluation` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33217d51",
   "metadata": {},
   "source": [
    "## RED 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86537ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 1\n",
    "number_evaluations = 50\n",
    "config_filename = 'configs/2b_red2_config.txt'\n",
    "log_directory = 'data/2b/red2/logs/'\n",
    "\n",
    "red2_run_and_log(red2_genetic_programming_search, number_evaluations, config_filename, log_directory+'1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c27d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 8\n",
    "number_evaluations = 5000\n",
    "config_filename = 'configs/2b_red2_config.txt'\n",
    "log_directory = 'data/2b/red2/logs/tuning/'\n",
    "\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Tuning runs can be called here\n",
    "def one_run(i):\n",
    "    red2_run_and_log(red2_genetic_programming_search, number_evaluations, config_filename, log_directory+str(i)+\".json\")\n",
    "with Pool() as pool:\n",
    "    pool.map(one_run, range(number_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d661b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'data/2b/red2/logs/tuning/'\n",
    "\n",
    "# for plotting\n",
    "evals = None\n",
    "total_penalized_means = None\n",
    "total_penalized_maxes = None\n",
    "total_base_means = None\n",
    "total_base_maxes = None\n",
    "\n",
    "# required and starts\n",
    "run_logs = []\n",
    "best_scores = []\n",
    "best_genes = []\n",
    "game_logs = []\n",
    "\n",
    "n = 0\n",
    "for file in os.listdir(log_directory):\n",
    "    if not file.endswith('.json'):\n",
    "        continue\n",
    "    data = None\n",
    "    with open(log_directory + file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        print(file, data['best_fitness'])\n",
    "        run_logs.append(data['gp_log'])\n",
    "        best_scores.append(data['best_fitness'])\n",
    "        best_genes.append(data['best_genes'])\n",
    "        game_logs.append(data['best_game_log'])\n",
    "        if not evals:\n",
    "            evals = data['evaluations']\n",
    "            total_penalized_means = data['mean_penalized_fitness']\n",
    "            total_penalized_maxes = data['best_penalized_fitness']\n",
    "            total_base_means = data['mean_base_fitness']\n",
    "            total_base_maxes = data['best_base_fitness']\n",
    "        else:\n",
    "            for i in range(len(evals)):\n",
    "                total_penalized_means[i] += data['mean_penalized_fitness'][i]\n",
    "                total_penalized_maxes[i] += data['best_penalized_fitness'][i]\n",
    "                total_base_means[i] += data['mean_base_fitness'][i]\n",
    "                total_base_maxes[i] += data['best_base_fitness'][i]\n",
    "    n += 1\n",
    "\n",
    "best_run = best_scores.index(max(best_scores))\n",
    "print(f\"Mean: {statistics.mean(best_scores)}; std: {statistics.stdev(best_scores)}\")\n",
    "print(\"Best run was\", best_run, \"with score\", max(best_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_penalized_means = [t/n for t in total_penalized_means]\n",
    "average_penalized_maxes = [t/n for t in total_penalized_maxes]\n",
    "average_base_means = [t/n for t in total_base_means]\n",
    "average_base_maxes = [t/n for t in total_base_maxes]\n",
    "evals = evals\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evals, average_penalized_maxes, 'g-')\n",
    "ax.plot(evals, average_penalized_means, 'g--')\n",
    "ax.plot(evals, average_base_maxes, 'b-')\n",
    "ax.plot(evals, average_base_means, 'b--')\n",
    "ax.set(xlabel = 'Evaluations', ylabel = 'Fitness',\\\n",
    "       title = f'2b Red2 Tuning: Evaluations vs Population Fitness Averaged Across {n} Runs')\n",
    "ax.legend(['Max Penalized Fitness', 'Mean Penalized Fitness', 'Max Base Fitness', 'Mean Base Fitness'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd429e89",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 10\n",
    "number_evaluations = 5000\n",
    "config_filename = 'configs/2b_red2_config.txt'\n",
    "log_directory = 'data/2b/red2/logs/'\n",
    "\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Tuning runs can be called here\n",
    "def one_run(i):\n",
    "    red2_run_and_log(red2_genetic_programming_search, number_evaluations, config_filename, log_directory+str(i)+\".json\")\n",
    "with Pool() as pool:\n",
    "    pool.map(one_run, range(number_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8258fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'data/2b/red2/logs/'\n",
    "\n",
    "# for plotting\n",
    "evals = None\n",
    "total_penalized_means = None\n",
    "total_penalized_maxes = None\n",
    "total_base_means = None\n",
    "total_base_maxes = None\n",
    "\n",
    "# required and starts\n",
    "run_logs = []\n",
    "best_scores = []\n",
    "best_genes = []\n",
    "game_logs = []\n",
    "\n",
    "n = 0\n",
    "for file in os.listdir(log_directory):\n",
    "    if not file.endswith('.json'):\n",
    "        continue\n",
    "    data = None\n",
    "    with open(log_directory + file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        print(file, data['best_fitness'])\n",
    "        run_logs.append(data['gp_log'])\n",
    "        best_scores.append(data['best_fitness'])\n",
    "        best_genes.append(data['best_genes'])\n",
    "        game_logs.append(data['best_game_log'])\n",
    "        if not evals:\n",
    "            evals = data['evaluations']\n",
    "            total_penalized_means = data['mean_penalized_fitness']\n",
    "            total_penalized_maxes = data['best_penalized_fitness']\n",
    "            total_base_means = data['mean_base_fitness']\n",
    "            total_base_maxes = data['best_base_fitness']\n",
    "        else:\n",
    "            for i in range(len(evals)):\n",
    "                total_penalized_means[i] += data['mean_penalized_fitness'][i]\n",
    "                total_penalized_maxes[i] += data['best_penalized_fitness'][i]\n",
    "                total_base_means[i] += data['mean_base_fitness'][i]\n",
    "                total_base_maxes[i] += data['best_base_fitness'][i]\n",
    "    n += 1\n",
    "\n",
    "best_run = best_scores.index(max(best_scores))\n",
    "print(\"Best run was\", best_run, \"with score\", max(best_scores), \"and genes \\n\", best_genes[best_run])\n",
    "with open(log_directory + 'best_game.txt', 'w') as f:\n",
    "    f.write('\\n'.join(game_logs[best_run]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef4daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign your EA's log member variables here\n",
    "# It should be a list containing the log from the EA in each run\n",
    "run_logs = run_logs\n",
    "experiment_directory = 'data/2b/red2/'\n",
    "\n",
    "# Writing the logs to files\n",
    "os.makedirs(f'{experiment_directory}logs/', exist_ok=True)\n",
    "for i in range(len(run_logs)):\n",
    "    with open(f'{experiment_directory}logs/' + str(i+1) + '.txt', 'w') as f:\n",
    "        f.write(''.join([entry + '\\n' for entry in run_logs[i]]))\n",
    "\n",
    "# Assign your data for statistical analysis to this variable\n",
    "# It should be a list of the highest fitness values seen per run\n",
    "stats_data = best_scores\n",
    "\n",
    "# Writing your statistical data to a file\n",
    "with open(f'{experiment_directory}/statistics.txt', 'w') as f:\n",
    "    for result in stats_data:\n",
    "        f.write(str(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_penalized_means = [t/n for t in total_penalized_means]\n",
    "average_penalized_maxes = [t/n for t in total_penalized_maxes]\n",
    "average_base_means = [t/n for t in total_base_means]\n",
    "average_base_maxes = [t/n for t in total_base_maxes]\n",
    "evals = evals\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evals, average_penalized_maxes, 'g-')\n",
    "ax.plot(evals, average_penalized_means, 'g--')\n",
    "ax.plot(evals, average_base_maxes, 'b-')\n",
    "ax.plot(evals, average_base_means, 'b--')\n",
    "ax.set(xlabel = 'Evaluations', ylabel = 'Fitness',\\\n",
    "       title = f'2b Red2: Evaluations vs Population Fitness Averaged Across {n} Runs')\n",
    "ax.legend(['Max Penalized Fitness', 'Mean Penalized Fitness', 'Max Base Fitness', 'Mean Base Fitness'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f66bf4",
   "metadata": {},
   "source": [
    "# RED3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b65b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 1\n",
    "number_evaluations = 50\n",
    "config_filename = 'configs/2b_red3_config.txt'\n",
    "log_directory = 'data/2b/red3/logs/'\n",
    "res = red3_genetic_programming_search(number_evaluations, config_filename)\n",
    "red3_run_and_log(lambda x,y: res, number_evaluations, config_filename, log_directory+'1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ef128",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(log_directory + 'tuning_game.txt', 'w') as f:\n",
    "    f.write('\\n'.join(res['best_game_log']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3819b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 8\n",
    "number_evaluations = 5000\n",
    "config_filename = 'configs/2b_red3_config.txt'\n",
    "log_directory = 'data/2b/red3/logs/tuning/'\n",
    "\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Tuning runs can be called here\n",
    "def one_run(i):\n",
    "    red3_run_and_log(red3_genetic_programming_search, number_evaluations, config_filename, log_directory+str(i)+\".json\")\n",
    "with Pool() as pool:\n",
    "    pool.map(one_run, range(number_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'data/2b/red3/logs/tuning/'\n",
    "\n",
    "# for plotting\n",
    "evals = None\n",
    "total_penalized_means = None\n",
    "total_penalized_maxes = None\n",
    "total_base_means = None\n",
    "total_base_maxes = None\n",
    "\n",
    "# required and starts\n",
    "run_logs = []\n",
    "best_scores = []\n",
    "best_genes = []\n",
    "game_logs = []\n",
    "\n",
    "n = 0\n",
    "for file in os.listdir(log_directory):\n",
    "    if not file.endswith('.json'):\n",
    "        continue\n",
    "    data = None\n",
    "    with open(log_directory + file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        print(file, data['best_fitness'])\n",
    "        run_logs.append(data['gp_log'])\n",
    "        best_scores.append(data['best_fitness'])\n",
    "        best_genes.append(data['best_3_genes'])\n",
    "        game_logs.append(data['best_game_log'])\n",
    "        if not evals:\n",
    "            evals = data['evaluations']\n",
    "            total_penalized_means = data['mean_penalized_fitness']\n",
    "            total_penalized_maxes = data['best_penalized_fitness']\n",
    "            total_base_means = data['mean_base_fitness']\n",
    "            total_base_maxes = data['best_base_fitness']\n",
    "        else:\n",
    "            for i in range(len(evals)):\n",
    "                total_penalized_means[i] += data['mean_penalized_fitness'][i]\n",
    "                total_penalized_maxes[i] += data['best_penalized_fitness'][i]\n",
    "                total_base_means[i] += data['mean_base_fitness'][i]\n",
    "                total_base_maxes[i] += data['best_base_fitness'][i]\n",
    "    n += 1\n",
    "\n",
    "best_run = best_scores.index(max(best_scores))\n",
    "print(f\"Mean: {statistics.mean(best_scores)}; std: {statistics.stdev(best_scores)}\")\n",
    "print(\"Best run was\", best_run, \"with score\", max(best_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8708af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_penalized_means = [t/n for t in total_penalized_means]\n",
    "average_penalized_maxes = [t/n for t in total_penalized_maxes]\n",
    "average_base_means = [t/n for t in total_base_means]\n",
    "average_base_maxes = [t/n for t in total_base_maxes]\n",
    "evals = evals\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evals, average_penalized_maxes, 'g-')\n",
    "ax.plot(evals, average_penalized_means, 'g--')\n",
    "ax.plot(evals, average_base_maxes, 'b-')\n",
    "ax.plot(evals, average_base_means, 'b--')\n",
    "ax.set(xlabel = 'Evaluations', ylabel = 'Fitness',\\\n",
    "       title = f'2b Red3 Tuning: Evaluations vs Population Fitness Averaged Across {n} Runs')\n",
    "ax.legend(['Max Penalized Fitness', 'Mean Penalized Fitness', 'Max Base Fitness', 'Mean Base Fitness'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655520cd",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae4afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 10\n",
    "number_evaluations = 5000\n",
    "config_filename = 'configs/2b_red3_config.txt'\n",
    "log_directory = 'data/2b/red3/logs/'\n",
    "\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Tuning runs can be called here\n",
    "def one_run(i):\n",
    "    red3_run_and_log(red3_genetic_programming_search, number_evaluations, config_filename, log_directory+str(i)+\".json\")\n",
    "with Pool() as pool:\n",
    "    pool.map(one_run, range(number_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de694de",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'data/2b/red3/logs/'\n",
    "\n",
    "# for plotting\n",
    "evals = None\n",
    "total_penalized_means = None\n",
    "total_penalized_maxes = None\n",
    "total_base_means = None\n",
    "total_base_maxes = None\n",
    "\n",
    "# required and starts\n",
    "run_logs = []\n",
    "best_scores = []\n",
    "best_genes = []\n",
    "game_logs = []\n",
    "\n",
    "n = 0\n",
    "for file in os.listdir(log_directory):\n",
    "    if not file.endswith('.json'):\n",
    "        continue\n",
    "    data = None\n",
    "    with open(log_directory + file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        print(file, data['best_fitness'])\n",
    "        run_logs.append(data['gp_log'])\n",
    "        best_scores.append(data['best_fitness'])\n",
    "        best_genes.append(data['best_3_genes'])\n",
    "        game_logs.append(data['best_game_log'])\n",
    "        if not evals:\n",
    "            evals = data['evaluations']\n",
    "            total_penalized_means = data['mean_penalized_fitness']\n",
    "            total_penalized_maxes = data['best_penalized_fitness']\n",
    "            total_base_means = data['mean_base_fitness']\n",
    "            total_base_maxes = data['best_base_fitness']\n",
    "        else:\n",
    "            for i in range(len(evals)):\n",
    "                total_penalized_means[i] += data['mean_penalized_fitness'][i]\n",
    "                total_penalized_maxes[i] += data['best_penalized_fitness'][i]\n",
    "                total_base_means[i] += data['mean_base_fitness'][i]\n",
    "                total_base_maxes[i] += data['best_base_fitness'][i]\n",
    "    n += 1\n",
    "\n",
    "best_run = best_scores.index(max(best_scores))\n",
    "print(\"Best run was\", best_run, \"with score\", max(best_scores), \"and genes \\n\", best_genes[best_run])\n",
    "with open(log_directory + 'best_game.txt', 'w') as f:\n",
    "    f.write('\\n'.join(game_logs[best_run]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign your EA's log member variables here\n",
    "# It should be a list containing the log from the EA in each run\n",
    "run_logs = run_logs\n",
    "experiment_directory = 'data/2b/red3/'\n",
    "\n",
    "# Writing the logs to files\n",
    "os.makedirs(f'{experiment_directory}logs/', exist_ok=True)\n",
    "for i in range(len(run_logs)):\n",
    "    with open(f'{experiment_directory}logs/' + str(i+1) + '.txt', 'w') as f:\n",
    "        f.write(''.join([entry + '\\n' for entry in run_logs[i]]))\n",
    "\n",
    "# Assign your data for statistical analysis to this variable\n",
    "# It should be a list of the highest fitness values seen per run\n",
    "stats_data = best_scores\n",
    "\n",
    "# Writing your statistical data to a file\n",
    "with open(f'{experiment_directory}/statistics.txt', 'w') as f:\n",
    "    for result in stats_data:\n",
    "        f.write(str(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c16bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_penalized_means = [t/n for t in total_penalized_means]\n",
    "average_penalized_maxes = [t/n for t in total_penalized_maxes]\n",
    "average_base_means = [t/n for t in total_base_means]\n",
    "average_base_maxes = [t/n for t in total_base_maxes]\n",
    "evals = evals\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evals, average_penalized_maxes, 'g-')\n",
    "ax.plot(evals, average_penalized_means, 'g--')\n",
    "ax.plot(evals, average_base_maxes, 'b-')\n",
    "ax.plot(evals, average_base_means, 'b--')\n",
    "ax.set(xlabel = 'Evaluations', ylabel = 'Fitness',\\\n",
    "       title = f'2b Red3: Evaluations vs Population Fitness Averaged Across {n} Runs')\n",
    "ax.legend(['Max Penalized Fitness', 'Mean Penalized Fitness', 'Max Base Fitness', 'Mean Base Fitness'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd325f",
   "metadata": {},
   "source": [
    "# RED 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac93db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 1\n",
    "number_evaluations = 50\n",
    "config_filename = 'configs/2b_red4_config.txt'\n",
    "log_directory = 'data/2b/red4/logs/'\n",
    "res = red4_genetic_programming_search(number_evaluations, config_filename)\n",
    "red4_run_and_log(lambda x,y: res, number_evaluations, config_filename, log_directory+'1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308c0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 8\n",
    "number_evaluations = 5000\n",
    "config_filename = 'configs/2b_red4_config.txt'\n",
    "log_directory = 'data/2b/red4/logs/tuning/'\n",
    "\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Tuning runs can be called here\n",
    "def one_run(i):\n",
    "    red4_run_and_log(red4_genetic_programming_search, number_evaluations, config_filename, log_directory+str(i)+\".json\")\n",
    "with Pool() as pool:\n",
    "    pool.map(one_run, range(number_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'data/2b/red4/logs/tuning/'\n",
    "\n",
    "# for plotting\n",
    "evals = None\n",
    "total_penalized_means = None\n",
    "total_penalized_maxes = None\n",
    "total_base_means = None\n",
    "total_base_maxes = None\n",
    "\n",
    "# required and starts\n",
    "run_logs = []\n",
    "best_scores = []\n",
    "best_genes = []\n",
    "game_logs = []\n",
    "\n",
    "n = 0\n",
    "for file in os.listdir(log_directory):\n",
    "    if not file.endswith('.json'):\n",
    "        continue\n",
    "    data = None\n",
    "    with open(log_directory + file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        print(file, data['best_fitness'])\n",
    "        run_logs.append(data['gp_log'])\n",
    "        best_scores.append(data['best_fitness'])\n",
    "        best_genes.append(data['best_genes'])\n",
    "        game_logs.append(data['best_game_log'])\n",
    "        if not evals:\n",
    "            evals = data['evaluations']\n",
    "            total_penalized_means = data['mean_penalized_fitness']\n",
    "            total_penalized_maxes = data['best_penalized_fitness']\n",
    "            total_base_means = data['mean_base_fitness']\n",
    "            total_base_maxes = data['best_base_fitness']\n",
    "        else:\n",
    "            for i in range(len(evals)):\n",
    "                total_penalized_means[i] += data['mean_penalized_fitness'][i]\n",
    "                total_penalized_maxes[i] += data['best_penalized_fitness'][i]\n",
    "                total_base_means[i] += data['mean_base_fitness'][i]\n",
    "                total_base_maxes[i] += data['best_base_fitness'][i]\n",
    "    n += 1\n",
    "\n",
    "best_run = best_scores.index(max(best_scores))\n",
    "print(f\"Mean: {statistics.mean(best_scores)}; std: {statistics.stdev(best_scores)}\")\n",
    "print(\"Best run was\", best_run, \"with score\", max(best_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db880b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_penalized_means = [t/n for t in total_penalized_means]\n",
    "average_penalized_maxes = [t/n for t in total_penalized_maxes]\n",
    "average_base_means = [t/n for t in total_base_means]\n",
    "average_base_maxes = [t/n for t in total_base_maxes]\n",
    "evals = evals\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evals, average_penalized_maxes, 'g-')\n",
    "ax.plot(evals, average_penalized_means, 'g--')\n",
    "ax.plot(evals, average_base_maxes, 'b-')\n",
    "ax.plot(evals, average_base_means, 'b--')\n",
    "ax.set(xlabel = 'Evaluations', ylabel = 'Fitness',\\\n",
    "       title = f'2b Red4 Tuning: Evaluations vs Population Fitness Averaged Across {n} Runs')\n",
    "ax.legend(['Max Penalized Fitness', 'Mean Penalized Fitness', 'Max Base Fitness', 'Mean Base Fitness'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239e93b1",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ef30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 10\n",
    "number_evaluations = 5000\n",
    "config_filename = 'configs/2b_red4_config.txt'\n",
    "log_directory = 'data/2b/red4/logs/'\n",
    "\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Tuning runs can be called here\n",
    "def one_run(i):\n",
    "    red4_run_and_log(red4_genetic_programming_search, number_evaluations, config_filename, log_directory+str(i)+\".json\")\n",
    "with Pool() as pool:\n",
    "    pool.map(one_run, range(number_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'data/2b/red4/logs/'\n",
    "\n",
    "# for plotting\n",
    "evals = None\n",
    "total_penalized_means = None\n",
    "total_penalized_maxes = None\n",
    "total_base_means = None\n",
    "total_base_maxes = None\n",
    "\n",
    "# required and starts\n",
    "run_logs = []\n",
    "best_scores = []\n",
    "best_genes = []\n",
    "game_logs = []\n",
    "\n",
    "n = 0\n",
    "for file in os.listdir(log_directory):\n",
    "    if not file.endswith('.json'):\n",
    "        continue\n",
    "    data = None\n",
    "    with open(log_directory + file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        print(file, data['best_fitness'])\n",
    "        run_logs.append(data['gp_log'])\n",
    "        best_scores.append(data['best_fitness'])\n",
    "        best_genes.append(data['best_genes'])\n",
    "        game_logs.append(data['best_game_log'])\n",
    "        if not evals:\n",
    "            evals = data['evaluations']\n",
    "            total_penalized_means = data['mean_penalized_fitness']\n",
    "            total_penalized_maxes = data['best_penalized_fitness']\n",
    "            total_base_means = data['mean_base_fitness']\n",
    "            total_base_maxes = data['best_base_fitness']\n",
    "        else:\n",
    "            for i in range(len(evals)):\n",
    "                total_penalized_means[i] += data['mean_penalized_fitness'][i]\n",
    "                total_penalized_maxes[i] += data['best_penalized_fitness'][i]\n",
    "                total_base_means[i] += data['mean_base_fitness'][i]\n",
    "                total_base_maxes[i] += data['best_base_fitness'][i]\n",
    "    n += 1\n",
    "\n",
    "best_run = best_scores.index(max(best_scores))\n",
    "print(\"Best run was\", best_run, \"with score\", max(best_scores), \"and genes \\n\", best_genes[best_run])\n",
    "with open(log_directory + 'best_game.txt', 'w') as f:\n",
    "    f.write('\\n'.join(game_logs[best_run]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdbfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign your EA's log member variables here\n",
    "# It should be a list containing the log from the EA in each run\n",
    "run_logs = run_logs\n",
    "experiment_directory = 'data/2b/red4/'\n",
    "\n",
    "# Writing the logs to files\n",
    "os.makedirs(f'{experiment_directory}logs/', exist_ok=True)\n",
    "for i in range(len(run_logs)):\n",
    "    with open(f'{experiment_directory}logs/' + str(i+1) + '.txt', 'w') as f:\n",
    "        f.write(''.join([entry + '\\n' for entry in run_logs[i]]))\n",
    "\n",
    "# Assign your data for statistical analysis to this variable\n",
    "# It should be a list of the highest fitness values seen per run\n",
    "stats_data = best_scores\n",
    "\n",
    "# Writing your statistical data to a file\n",
    "with open(f'{experiment_directory}/statistics.txt', 'w') as f:\n",
    "    for result in stats_data:\n",
    "        f.write(str(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f058f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_penalized_means = [t/n for t in total_penalized_means]\n",
    "average_penalized_maxes = [t/n for t in total_penalized_maxes]\n",
    "average_base_means = [t/n for t in total_base_means]\n",
    "average_base_maxes = [t/n for t in total_base_maxes]\n",
    "evals = evals\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evals, average_penalized_maxes, 'g-')\n",
    "ax.plot(evals, average_penalized_means, 'g--')\n",
    "ax.plot(evals, average_base_maxes, 'b-')\n",
    "ax.plot(evals, average_base_means, 'b--')\n",
    "ax.set(xlabel = 'Evaluations', ylabel = 'Fitness',\\\n",
    "       title = f'2b Red4: Evaluations vs Population Fitness Averaged Across {n} Runs')\n",
    "ax.legend(['Max Penalized Fitness', 'Mean Penalized Fitness', 'Max Base Fitness', 'Mean Base Fitness'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ab61c6",
   "metadata": {},
   "source": [
    "# RED 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba1bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 1\n",
    "number_evaluations = 60\n",
    "config_filename = 'configs/2b_red5_config.txt'\n",
    "log_directory = 'data/2b/red5/logs/'\n",
    "res = red5_genetic_programming_search(number_evaluations, config_filename)\n",
    "red5_run_and_log(lambda x,y: res, number_evaluations, config_filename, log_directory+'1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(log_directory + 'tuning_game.txt', 'w') as f:\n",
    "    f.write('\\n'.join(res['best_game_log']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1908383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 8\n",
    "number_evaluations = 5000\n",
    "config_filename = 'configs/2b_red5_config.txt'\n",
    "log_directory = 'data/2b/red5/logs/tuning/'\n",
    "\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Tuning runs can be called here\n",
    "def one_run(i):\n",
    "    red5_run_and_log(red5_genetic_programming_search, number_evaluations, config_filename, log_directory+str(i)+\".json\")\n",
    "with Pool() as pool:\n",
    "    pool.map(one_run, range(number_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ff135",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'data/2b/red5/logs/tuning/'\n",
    "\n",
    "# for plotting\n",
    "evals = None\n",
    "total_penalized_means = None\n",
    "total_penalized_maxes = None\n",
    "total_base_means = None\n",
    "total_base_maxes = None\n",
    "\n",
    "# required and starts\n",
    "run_logs = []\n",
    "best_scores = []\n",
    "best_genes = []\n",
    "game_logs = []\n",
    "\n",
    "n = 0\n",
    "for file in os.listdir(log_directory):\n",
    "    if not file.endswith('.json'):\n",
    "        continue\n",
    "    data = None\n",
    "    with open(log_directory + file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        print(file, data['best_fitness'])\n",
    "        run_logs.append(data['gp_log'])\n",
    "        best_scores.append(data['best_fitness'])\n",
    "        best_genes.append(data['best_3_genes'])\n",
    "        game_logs.append(data['best_game_log'])\n",
    "        if not evals:\n",
    "            evals = data['evaluations']\n",
    "            total_penalized_means = data['mean_penalized_fitness']\n",
    "            total_penalized_maxes = data['best_penalized_fitness']\n",
    "            total_base_means = data['mean_base_fitness']\n",
    "            total_base_maxes = data['best_base_fitness']\n",
    "        else:\n",
    "            for i in range(len(evals)):\n",
    "                total_penalized_means[i] += data['mean_penalized_fitness'][i]\n",
    "                total_penalized_maxes[i] += data['best_penalized_fitness'][i]\n",
    "                total_base_means[i] += data['mean_base_fitness'][i]\n",
    "                total_base_maxes[i] += data['best_base_fitness'][i]\n",
    "    n += 1\n",
    "\n",
    "best_run = best_scores.index(max(best_scores))\n",
    "print(f\"Mean: {statistics.mean(best_scores)}; std: {statistics.stdev(best_scores)}\")\n",
    "print(\"Best run was\", best_run, \"with score\", max(best_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_penalized_means = [t/n for t in total_penalized_means]\n",
    "average_penalized_maxes = [t/n for t in total_penalized_maxes]\n",
    "average_base_means = [t/n for t in total_base_means]\n",
    "average_base_maxes = [t/n for t in total_base_maxes]\n",
    "evals = evals\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evals, average_penalized_maxes, 'g-')\n",
    "ax.plot(evals, average_penalized_means, 'g--')\n",
    "ax.plot(evals, average_base_maxes, 'b-')\n",
    "ax.plot(evals, average_base_means, 'b--')\n",
    "ax.set(xlabel = 'Evaluations', ylabel = 'Fitness',\\\n",
    "       title = f'2b Red5 Tuning: Evaluations vs Population Fitness Averaged Across {n} Runs')\n",
    "ax.legend(['Max Penalized Fitness', 'Mean Penalized Fitness', 'Max Base Fitness', 'Mean Base Fitness'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35317524",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these values and re-run this cell as much as you'd like\n",
    "number_runs = 10\n",
    "number_evaluations = 5000\n",
    "config_filename = 'configs/2b_red5_config.txt'\n",
    "log_directory = 'data/2b/red5/logs/'\n",
    "\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Tuning runs can be called here\n",
    "def one_run(i):\n",
    "    red5_run_and_log(red5_genetic_programming_search, number_evaluations, config_filename, log_directory+str(i)+\".json\")\n",
    "with Pool() as pool:\n",
    "    pool.map(one_run, range(number_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'data/2b/red3/logs/'\n",
    "\n",
    "# for plotting\n",
    "evals = None\n",
    "total_penalized_means = None\n",
    "total_penalized_maxes = None\n",
    "total_base_means = None\n",
    "total_base_maxes = None\n",
    "\n",
    "# required and starts\n",
    "run_logs = []\n",
    "best_scores = []\n",
    "best_genes = []\n",
    "game_logs = []\n",
    "\n",
    "n = 0\n",
    "for file in os.listdir(log_directory):\n",
    "    if not file.endswith('.json'):\n",
    "        continue\n",
    "    data = None\n",
    "    with open(log_directory + file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        print(file, data['best_fitness'])\n",
    "        run_logs.append(data['gp_log'])\n",
    "        best_scores.append(data['best_fitness'])\n",
    "        best_genes.append(data['best_3_genes'])\n",
    "        game_logs.append(data['best_game_log'])\n",
    "        if not evals:\n",
    "            evals = data['evaluations']\n",
    "            total_penalized_means = data['mean_penalized_fitness']\n",
    "            total_penalized_maxes = data['best_penalized_fitness']\n",
    "            total_base_means = data['mean_base_fitness']\n",
    "            total_base_maxes = data['best_base_fitness']\n",
    "        else:\n",
    "            for i in range(len(evals)):\n",
    "                total_penalized_means[i] += data['mean_penalized_fitness'][i]\n",
    "                total_penalized_maxes[i] += data['best_penalized_fitness'][i]\n",
    "                total_base_means[i] += data['mean_base_fitness'][i]\n",
    "                total_base_maxes[i] += data['best_base_fitness'][i]\n",
    "    n += 1\n",
    "\n",
    "best_run = best_scores.index(max(best_scores))\n",
    "print(\"Best run was\", best_run, \"with score\", max(best_scores), \"and genes \\n\", best_genes[best_run])\n",
    "with open(log_directory + 'best_game.txt', 'w') as f:\n",
    "    f.write('\\n'.join(game_logs[best_run]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign your EA's log member variables here\n",
    "# It should be a list containing the log from the EA in each run\n",
    "run_logs = run_logs\n",
    "experiment_directory = 'data/2b/red5/'\n",
    "\n",
    "# Writing the logs to files\n",
    "os.makedirs(f'{experiment_directory}logs/', exist_ok=True)\n",
    "for i in range(len(run_logs)):\n",
    "    with open(f'{experiment_directory}logs/' + str(i+1) + '.txt', 'w') as f:\n",
    "        f.write(''.join([entry + '\\n' for entry in run_logs[i]]))\n",
    "\n",
    "# Assign your data for statistical analysis to this variable\n",
    "# It should be a list of the highest fitness values seen per run\n",
    "stats_data = best_scores\n",
    "\n",
    "# Writing your statistical data to a file\n",
    "with open(f'{experiment_directory}/statistics.txt', 'w') as f:\n",
    "    for result in stats_data:\n",
    "        f.write(str(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_penalized_means = [t/n for t in total_penalized_means]\n",
    "average_penalized_maxes = [t/n for t in total_penalized_maxes]\n",
    "average_base_means = [t/n for t in total_base_means]\n",
    "average_base_maxes = [t/n for t in total_base_maxes]\n",
    "evals = evals\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evals, average_penalized_maxes, 'g-')\n",
    "ax.plot(evals, average_penalized_means, 'g--')\n",
    "ax.plot(evals, average_base_maxes, 'b-')\n",
    "ax.plot(evals, average_base_means, 'b--')\n",
    "ax.set(xlabel = 'Evaluations', ylabel = 'Fitness',\\\n",
    "       title = f'2b Red5: Evaluations vs Population Fitness Averaged Across {n} Runs')\n",
    "ax.legend(['Max Penalized Fitness', 'Mean Penalized Fitness', 'Max Base Fitness', 'Mean Base Fitness'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00547606",
   "metadata": {},
   "source": [
    "# Side Note: Canonical Genetic Programming\n",
    "Should you apply GP after this class, you should know that the GP algorithm taught in this class (as described in the course textbook) differs somewhat from the algorithm canonically used in GP. Notably, the textbook has certain important omissions regarding the Ramped Half-and-half algorithm and the GP evolutionary cycle. In practice, the Ramped Half-and-half algorithm uses a `grow` method which ensures at least 1 branch reaches the depth limit. This can be difficult to implement, and has little impact on this assignment, so we don't require the implementation of the canonical version of the algorithm.\n",
    "\n",
    "More importantly, however, is that the canonical GP evolutionary cycle is generational in nature. In the canonical Generational GP algorithm, $\\mu$ children are created each generation via recombination, mutation, or *reproduction* and the children directly replace the parents without survival selection. Notably, the reproduction operation involves directly copying the selected parent to the children pool and this enables behavior similar to the steady-state survival of a $(\\mu + \\lambda)$-EA. The algorithm you implement for this assignment series is much more similar to a Genetic Algorithm in nature and you should be aware of this distinction if you continue to work with GP. For more information, see [here](https://geneticprogramming.com/about-gp/gp-workflow/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
